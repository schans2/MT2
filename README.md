# Music & Tech 2 Project Blog - Sam Schantz

## Final Update (5/3/22)
Alright, here we are with less than two hours to go before this project is due. While I unfortunately did not get to work on everything I had detailed in my demo presentation in class, I am still happy overall with how the project turned out. If I ever make a personal website, which I hope to do someday, I think this would be a neat project to showcase on there, and, in that case, I would almost certainly continue to work on extra features to realize the application to its full potential. What I have here for now, though, can be viewed in [this video](https://www.youtube.com/watch?v=89MxVTRuwJQ). It's been a pleasure working on *Seq & Destroy*, and I sincerely hope you enjoy using it as much as I did making it!

## Goal Reevaluation (4/10/22)
As discussed in class, I figured we should take another look at this project's timeline as we near the end of the semester. I still think I can complete the [minimum viable product](https://github.com/schans2/MT2/milestone/1), as I recently committed code to the `structural-logistics` branch that handles most of the under-the-hood logic behind getting instrument sequences to play in time with a global sense of tempo. By comparison, I believe that keyboard support for a user-controlled instrument and a basic audio mixing interface should be trivial to implement from a programming standpoint. As for other non-MVP features that I think are feasible, I think it's still possible for me to add audio timbre customization controls and implement recording, as, unlike the tempo-based sequencing I've been working on, WadJS should have most of the "heavy lifting" for these already abstracted away for easier implementation. Also, while I doubt I'll have time to design a "flashy" UI, I hope to make my final product at least look somewhat presentable.

## Visual Demo (3/28/22)
Alright, I have made a quick video of what I have for sequencing so far per request. [Go watch it here!](https://www.youtube.com/watch?v=hla3Fi7SIHA) Obviously it's not perfect, but I feel a lot of "under the hood" foundations are well in place to be at this point. **Question:** *How should I handle continued notes VS 32nd notes on the grid?* I'm thinking that for now, lines can be considered continued notes, since 16ths are often enough for separate articulations. With the data structure we talked about, as it's sequenced through, I imagine the code would look something like `if(sequence[i+1] exists && sequence[i+1] == sequence[i]) { continue }` -- checking to see if a note was indexed in the next column and that it is the same note value as the current one being played. Later this may be something I address better by adding in more data attributes and/or user control that distinguishes articulation.

## Progress on Sequencing (3/20/22)
At the time of writing here, I have made some solid progress on getting mouse-based sequencing input working. Notable accomplishments include having distinct saved sequencing states for both multiple instruments and multiple octaves (5 for now). I hope to make a video demo of this in action soon, but first I would like to implement the final major component of [this sequencing issue](https://github.com/schans2/MT2/issues/1), which is support for multiple measures, so I can close it alongside the creation of this demo. I am thinking of supporting as many measures as the user wants for a sequence loop through jQuery's fairly simple dynamic cloning library, however I am slightly concerned about performance here. As is, there are *a lot* of `div`s being rendered in this app (12 notes x 5 octaves x 32nd note support x 4 instruments (temp amount) = 7,680 `div`s). Say someone sequences a 16-measure loop in each instrument -- you're looking at over 100K `div`s right there each holding what instrument, octave, note, and beat count they belong to while either being marked active or inactive from user input. Now, in defense of this sort of naive solution, as a global tempo clock (or whatever you wanna call it) parses through each "tick" of this whole schema, it should only be (12 notes x 5 octaves x 4 instruments = 240 `div`s) being looked at per tick. Since this does not *have* to be a deployed program or anything like that, for now, I am willing to proceed down this road and see if my computer can handle it. I am optimistic in this regard, since, as my calculations show, this solution is more space-complex than it should be in terms of actual execution, and, at the end of the day, proccesing complexity should hold far more of a bearing over audio latency in my estimation than space complexity in RAM.
**Commits related to this update are as follows:**
1. [Click & Drag Control Scheme Working](https://github.com/schans2/MT2/commit/d16f1ff50171b196d1d89c9796df4b4227e4149c)
2. [Multi-Instrument Sequencing State Tracking](https://github.com/schans2/MT2/commit/88dde70a0f600cc83e5a90aab613b641365db339)
3. [Multi-Octave Support](https://github.com/schans2/MT2/commit/08361abc0d2a1f0dfe9c1926023812a9e21f057c)

## Project Timeline (3/2/22)
I have used GitHub's issue tracking and milestone features to outline what general tasks still need to be done for this project, what "tier" of project completion each of these falls into, and roughly when I would want to be done with each of these tiers under ideal circumstances. Of course, circumstances are rarely "ideal," hence the existence of these tiers in the first place. They are:
1. [*Minimum Viable Product:*](https://github.com/schans2/MT2/milestone/1) - These issues ***must*** be resolved to deliver on the project's fundamental premises/promises.
2. [*Functional Difference Makers*](https://github.com/schans2/MT2/milestone/2) - These are issues that, while not *strictly* essential, I feel would make the final product feel more complete if resolved. I still hope to complete most of this tier.
3. [*Cherries on Top*](https://github.com/schans2/MT2/milestone/3) - This tier holds what could be considered this project's "stretch goals," i.e. tasks that really only serve to further polish the product. None of these are core to its functionality, and as such these are comparatively deprioritized.

## Modal Madness (2/21/22)
A couple days ago, I tried to get started with implementing the click-based sequencing interface element I have discussed in the past. I figured an in-tab pop-up window, otherwise known as a modal, was the best way to go about this and, as such, I decided to start messing around with a modal plugin called [jBox](https://www.npmjs.com/package/jbox). I liked the impression I got from jBox when looking for modal solutions due to its popularity and extensive [demoing and documentation](https://stephanwagner.me/jBox). The mess I have made of things currently trying to get this thing to play nice with my CSS grid guidelines that I configured for the sequencing grid (12 notes per octave by 32 slots to support up to 32nd notes) can be seen in its full "glory" in `structural-logistics` at this time, though, as well as in the below screenshots. Said screenshots illustrate the bug I am currently hung up on, where the grid renders more or less as intended when placed on the page, however, the exact same combination of HTML and CSS completely crumbles when placed in the modal, with all of the bottom borders seeming to consolidate into the large black chunks that can be seen. Perhas I need to dig deeper into what exactly jBox is *doing* when it renders its modal -- what HTML structuring and CSS properties could it be applying that may be causing this? I hope to find the answer to this question sometime this week, but I may have to seek other modal solutions or maybe even write my own if I ultimately cannot resolve this. ![CSS Sequencing Grid Working on Page](/assets/blog/ScreenshotPageGrid.png "Working Grid on Page") ![CSS Sequencing Grid Broken in Modal](/assets/blog/ScreenshotModGrid.png "Broken Grid in Modal")

## Playing Around (2/11/22)
Well, I've finally started getting my hands dirty with [WadJS](https://www.npmjs.com/package/web-audio-daw) and I think I'm gonna stick with it. It seems pretty intuitive to pass values around from the interface to it, especially thanks to technologies like jQuery. What I have is pretty barebones so far, but it was an important step to get the module installed and get a basic grasp of its syntax and methods. Regarding said methods, though, there's still so much more I have yet to try that I'm looking forward to messing with outside of basic oscillators, such as playing WAV files, adding real-time audio effects, and testing recording capabilities. There's a good chance that what I'm tinkering with right now won't directly be used in the final implementation of this project and, as such, I've sectioned it off to its own branch called `wad-playground`.
I also created `structural-logistics` to serve as a place where I can start working on shaping interface elements. Yes, I know it was discussed in class that the audio comes before the visual, however I believe them to be fairly equally important in the context of web technologies, as interface elements are still responsible for transmitting data. The idea is that facets of the data transmission process can be set up here, then the WadJS API can be incorporated into this structure. This is a workflow I feel fairly confident about after getting to experience how simple and modular the API really is. Also, this gives me a segment of tasks to work towards when I am in settings where I cannot test audio.

## Supporting Research (1/31/22)
After querying the International Computer Music Association's research database to find relevant supporting literature for this project, I believe the following three sources can be informative:
- If mobile compatibility ends up making it into the scope of this project, [this paper](https://quod.lib.umich.edu/i/icmc/bbp2372.2011.086/1/--control-software-for-end-user-interface-programming?page=root;rgn=full+text;size=150;view=text) should prove quite helpful. It goes over a mobile application designed to send OSC messages through an interface that can be designed as fit using web technologies I am already familiar with.
- [This paper](https://quod.lib.umich.edu/i/icmc/bbp2372.2012.059/1/--real-time-web-technologies-in-the-networked-performance?page=root;rgn=full+text;size=150;view=text) should be more informative in a general sense for the type of project I am planning on making, as it goes over how some of the technologies I am interested in can be utilized.
- Finally, more information on some of the extended functionality of the Web Audio API can be found [here](https://quod.lib.umich.edu/i/icmc/bbp2372.2018.021/1/--audioworklet-the-future-of-web-audio?page=root;rgn=full+text;size=150;view=image). This could potentially prove useful if I end up electing to go with the direct Web Audio API (as opposed to some sort of NPM module abstraction of it) and wish to implement flashier features, such as multithreading for better performance with multiple "instruments".

## More Thinking About Technologies & UI/UX (1/27/22)
I am now quite interested in using [this NPM module](https://www.npmjs.com/package/web-audio-daw) for the sort of web instrument / DAW I want to make. My goal with this project is to implement as many DAW-like features as I can in the time I have on top of multiple playable or sequenceable virtual instruments. As such, I think this module will take a lot of the legwork out of implementing features I was worried about being complex, such as recording. Other notable links I want to put here are things that the module's page recommends for extended functionality, such as [these impulse responses for reverb](https://www.voxengo.com/impulses/) and [Tuna](https://github.com/Theodeus/tuna/wiki#the-nodes), a Web Audio effects library. Also here's some [Web Audio API documentation](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API), because why not?

_____

Regarding user interface, I am thinking about having the sequencing GUI resemble [Google Song Maker](https://musiclab.chromeexperiments.com/Song-Maker/), where you can click or touch the screen to input notes / drum patterns. Differences over this specifically I want to implement include having the grid to support up to 32nd notes, having some kind of triplet support, and having alternate time signature support (at least 3/4, 4/4, and 5/4, since you can extrapolate a good amount from these, but hopefully more uncommon ones too). As for the general "vibe" of the GUI, I am planning on going with the same black and rainbow aesthetic as my Music & Tech 1 final project, pictured. ![The Instant Dance Party](/assets/blog/ScreenshotIDP.png "The Instant Dance Party")

## About (1/13/22)
This blog is for a class I am currently taking: Music and Technology II. I hope it will fully and accurately convey whatever the timeline ends up being for this project's formation. For now, though, the creation of this blog certainly is a first step in that direction. As for project ideas, right now I'm thinking that I want to build a web-interface instrument of some kind. The term "instrument" might be just a bit disingenuous, though, as I'm imagining something you could play a full, multi-faceted song on. Going forward, I'll be thinking a lot about what sort of control sceme I should set up. Should the user perhaps be able to pre-program some kind of sequence for a bass and drum pattern and then play the lead, for instance? Or should there be multiple, simultaneously controllable timbres for the user? These are the kinds of things I'll be thinking about going forward.